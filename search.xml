<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pytorch上手， 使用pytorch对叶子分类]]></title>
    <url>%2F2017%2F06%2F09%2Fleafnet%2F</url>
    <content type="text"><![CDATA[项目介绍该项目是kaggle上的一个项目。数据集包含1584张二值化图片，包含训练集和测试集，除此之外，网站还提供了叶子的形状，纹理等共192维的属性特征。本实验使用卷积神经网络和对属性特征进行融合进行分类。项目地址是EvilGod’s Github，本实验没有做数据增强等，最后的score是0.06676，比较一般，有兴趣的可以自己去调网络及参数。 代码解读data_attr.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667## 从https://www.kaggle.com/c/leaf-classification/data 下载数据## 解压到 ./dataset/## 模仿 https://www.kaggle.com/abhmul/keras-convnet-lb-0-0052-w-visualization处理数据，具体是把先把长的边## resize到96，短边同scale变化，再将短边填充至96.完成resize到96*96的数据预处理。## 处理后的数据放在 ./dataset/leaves/ 文件夹，其文件结构如下'''- train - Acer_Capillipes - 0201.jpg - 0227.jpg - ... - Acer_Circinatum - ****.jpg val - Acer_Capillipes - ****.jpg - Acer_Circinatum - ****.jpg- test - 000 0004.jpg## 可以从'http://pan.baidu.com/s/1hrJNsEC'下载处理后的数据集''' import osimport numpy as npimport pandas as pdfrom sklearn.preprocessing import LabelEncoderfrom sklearn.preprocessing import StandardScalerimport scipy.io as siodata_path = './dataset'# 从csv文件加载训练的属性特征def load_training_att(): data = pd.read_csv(os.path.join(data_path,'train.csv')) ID =data.pop('id') y = data.pop('species') y = LabelEncoder().fit(y).transform(y) x = StandardScaler().fit(data).transform(data) att = &#123;&#125; for i, ids in enumerate(ID, 0): name = '%04d.jpg'%ids att[name] = x[i,:].astype(np.float32) return att # 从csv文件加载测试的属性特征def load_test_att(att_train): data = pd.read_csv(os.path.join(data_path,'test.csv')) ID =data.pop('id') x = StandardScaler().fit(data).transform(data) for i, ids in enumerate(ID, 0): name = '%04d.jpg'%ids att_train[name] = x[i,:].astype(np.float32) return att if __name__ == '__main__': att = load_training_att() att = load_test_att(att) print len(att) sio.savemat('dataset/att.mat', att) model.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import torchimport torch.nn as nnfrom torch.nn.init import xavier_normal# 网络参数初始化 xavierdef xavier_init(m): classname = m.__class__.__name__ if classname.find('Conv') != -1: xavier_normal(m.weight.data) elif classname.find('Linear') != -1: xavier_normal(m.weight.data) m.bias.data.fill_(1)# 网络参数初始化 高斯分布 def weights_init(m): classname = m.__class__.__name__ if classname.find('Conv') != -1: m.weight.data.normal_(0.0, 0.02) elif classname.find('BatchNorm') != -1: m.weight.data.normal_(1.0, 0.02) m.bias.data.fill_(0) # # 网络定义，参考# https://www.kaggle.com/abhmul/keras-convnet-lb-0-0052-w-visualizationclass leafnet(nn.Module): def __init__(self, nc): super(leafnet, self).__init__() self.nc = nc self.conv = nn.Sequential( nn.Conv2d(self.nc, 8, 5, bias=False), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(8, 32, 5, bias=False), nn.ReLU(), nn.MaxPool2d(2, 2) ) self.fc = nn.Sequential( nn.Linear(14304, 1000), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1000, 99) # nn.CrossEntropyLoss() does not need Softmax # nn.Softmax() ) def forward(self, input_img, input_att, nb): img_conv = self.conv.forward(input_img) img_flat = img_conv.view(nb, -1) #融合图像和属性特征 x = torch.cat((img_flat, input_att), 1) ##fully connect output = self.fc(x) return output data_attr.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 根据 torchvision.datasets.folder.py修改import torch.utils.data as datafrom PIL import Imageimport osimport os.pathimport scipy.io as sioIMG_EXTENSIONS = [ '.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',]def is_image_file(filename): return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)def find_classes(dir): classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))] classes.sort() class_to_idx = &#123;classes[i]: i for i in range(len(classes))&#125; return classes, class_to_idxdef make_dataset(dir, class_to_idx): images = [] for target in os.listdir(dir): d = os.path.join(dir, target) if not os.path.isdir(d): continue for root, _, fnames in sorted(os.walk(d)): for fname in fnames: if is_image_file(fname): path = os.path.join(root, fname) item = (path, class_to_idx[target]) images.append(item) return imagesdef pil_loader(path): # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835) with open(path, 'rb') as f: with Image.open(f) as img: ##### 1. 转成灰度图 ##### return img.convert('L')def accimage_loader(path): import accimage try: return accimage.Image(path) except IOError: # Potentially a decoding problem, fall back to PIL.Image return pil_loader(path)def default_loader(path): return pil_loader(path)class ImageFolder(data.Dataset): def __init__(self, root, transform=None, target_transform=None, loader=default_loader): classes, class_to_idx = find_classes(root) imgs = make_dataset(root, class_to_idx) if len(imgs) == 0: raise(RuntimeError("Found 0 images in subfolders of: " + root + "\n" "Supported image extensions are: " + ",".join(IMG_EXTENSIONS))) self.root = root self.imgs = imgs self.classes = classes self.class_to_idx = class_to_idx self.transform = transform self.target_transform = target_transform self.loader = loader ##### 2. 加载属性特征 ##### self.input_atts = sio.loadmat('./dataset/att.mat') def __getitem__(self, index): """ Args: index (int): Index Returns: tuple: (image, target) where target is class_index of the target class. """ path, target = self.imgs[index] img = self.loader(path) ##### 3. 根据名字得到属性特征 ##### input_att = self.input_atts[path.split('/')[-1]] if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) ##### 4. 返回 ##### return img, input_att, target def __len__(self): return len(self.imgs) main.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198import randomimport cv2import torchimport torch.nn as nnimport torch.optim as optimimport torch.utils.dataimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torch.backends.cudnn as cudnnfrom torch.autograd import Variableimport torch.nn.functional as functionalimport modelfrom model import xavier_initfrom model import weights_initfrom myfolder import ImageFolderfrom PIL import Imageimport numpy as npimport scipy.io as sioimport csvdata_path = './dataset/leaves'#将array转成uint8，在jupyter notebook下查看def arr2unit8(arr): #arr m*n# img_numpy = input.data[0,:,:,:].cpu().numpy()[0,:,:] variable res = np.zeros((arr.shape[0],arr.shape[1]),dtype=np.uint8) for i in range(arr.shape[0]): for j in range(arr.shape[1]): if arr[i][j] &gt; 0.5: res[i][j] = 255 return res # 保存提交结果class leaf_save: def __init__(self, classes, filename='sample_submission.csv'): first_row = ['id'] first_row.extend(classes) self.csvfile = file(filename, 'wb') self.csv_writer = csv.writer(self.csvfile) self.csv_writer.writerow(first_row) def write_data(self, result): self.csv_writer.writerows(result) def close(self): self.csvfile.close() if __name__ == "__main__": # 训练或测试模式 phase = 'test'# phase = 'test' #batch size nb = 128 is_cuda = True random.seed(random.randint(1,10000)) torch.manual_seed(random.randint(1,10000)) pretrain_model = 'models/epoch_49.pth' #初始化模型，训练时设为'' if phase == 'train': pretrain_model = '' #use cudnn cudnn.benchmark = True #Normalized to (0,1) normalization = transforms.Normalize((0., 0., 0.),(1., 1., 1.))# normalization = transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)) #读取数据 if phase == 'train': trainset = ImageFolder(root=data_path+'/train', \ transform=transforms.Compose([\ transforms.ToTensor(), normalization ])) train_loader = torch.utils.data.DataLoader(trainset, batch_size = nb,\ shuffle=True, num_workers = 2) valset = ImageFolder(root=data_path+'/val', \ transform=transforms.Compose([\ transforms.ToTensor(), normalization ])) #198 all val images val_loader = torch.utils.data.DataLoader(valset, batch_size = 198,\ shuffle=True, num_workers = 2) if phase == 'test': testset = ImageFolder(root=data_path+'/test', \ transform=transforms.Compose([\ transforms.ToTensor(), normalization ])) test_loader = torch.utils.data.DataLoader(testset, batch_size = 1,\ shuffle=False, num_workers = 1) # network net = model.leafnet(1) net.apply(xavier_init) if pretrain_model != '': print 'loading pretrained model' net.load_state_dict(torch.load(pretrain_model)) criterion = nn.CrossEntropyLoss() input = torch.FloatTensor(nb, 1, 96, 96) label = torch.LongTensor(nb) input_att = torch.FloatTensor(nb, 192) if is_cuda: net.cuda() criterion.cuda() input = input.cuda() label = label.cuda() input_att = input_att.cuda() input = Variable(input) label = Variable(label) input_att = Variable(input_att) #使用Adam优化，效果比较好# optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.9)# optimizer = optim.RMSprop(net.parameters(), lr = 0.01, momentum=0.9) optimizer = optim.Adam(net.parameters(), lr=0.01, betas = (.5, 0.999)) ## train total_epoch = 50 prev_loss = 10000 if phase == 'train': for epoch in range(total_epoch): if (epoch + 1)%10 ==0: for param_group in optimizer.param_groups: param_group['lr'] = param_group['lr'] * 0.1 for i, (imgs, atts, labels) in enumerate(train_loader, 0): nb = labels.size(0) ##将梯度重置 0，不然回传时梯度会累加 net.zero_grad() input.data.resize_(imgs.size()).copy_(imgs) label.data.resize_(nb).copy_(labels) input_att.data.resize_(atts.view(-1, 192).size()).copy_(atts.view(-1, 192)) output = net(input, input_att, nb) loss = criterion(output, label) optimizer.zero_grad() loss.backward() optimizer.step() print 'Epoch: %d, iter:%d, Train loss is %f'%(epoch, i, loss.data[0]) # 在验证集上看loss变化 for i, (imgs, atts, labels) in enumerate(val_loader, 0): nb = labels.size(0) input.data.resize_(imgs.size()).copy_(imgs) label.data.resize_(nb).copy_(labels) input_att.data.resize_(atts.view(-1, 192).size()).copy_(atts.view(-1, 192)) output = net(input, input_att, nb) loss = criterion(output, label) print '---------------------------------------------' print 'Epoch: %d, Val loss is %f, Loss decreases %f'%( epoch, loss.data[0], prev_loss - loss.data[0]) print '---------------------------------------------' prev_loss = loss.data[0] if epoch % 20 == 0 or epoch == total_epoch - 1: torch.save(net.state_dict(), 'models/epoch_%d.pth'%(epoch)) else: #保存提交结果 submission = leaf_save(trainset.classes) imgs_test = [] imgs_class = [] results = [] imgs_name = testset.imgs for i, (imgs, atts, _) in enumerate(test_loader, 0): result_i = [] input.data.resize_(imgs.size()).copy_(imgs) origin_im = arr2unit8(input.data[0,:,:,:].cpu().numpy()[0,:,:]) origin_im = Image.fromarray(origin_im) input_att.data.resize_(atts.view(-1, 192).size()).copy_(atts.view(-1, 192)) output = net(input, input_att, 1) output = functional.softmax(output).data.cpu().numpy() max_idx = np.where(output.reshape(-1)== max(output.reshape(-1)))[0][0] pred_class = trainset.classes[max_idx] img_name = imgs_name[i][0].split('/')[-1] idx = str(int(img_name[0:img_name.find('.')])) ### write to csv result_i.append(idx) for op in output.reshape(99,): result_i.append('%.6f'%op) results.append(tuple(result_i)) ###供可视化用 imgs_test.append(origin_im) imgs_class.append(pred_class) #得到的csv文件记得对ID号排序再提交 submission.write_data(results) submission.close() print 'finised' 可视化 Reference[1] AbhijeetMulgundKeras: ConvNet LB 0.0052 w/ Visualization]]></content>
      <tags>
        <tag>pytorch</tag>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装或运行Caffe遇到的一些问题]]></title>
    <url>%2F2017%2F06%2F04%2Fcaffe-issues%2F</url>
    <content type="text"><![CDATA[所有问题形如“undefined reference to TIFFRGBAImageOK@LIBTIFF_4.0”的问题。 参考opencv论坛的方法 如果装了anaconda的话，可能与anaconda的tiff库冲突，使用下列命令1conda remove libtiff No module named caffe1export PYTHONPATH=$CAFFEROOT/python:$PYTHONPATH ImportError: libcudart.so.*.0: cannot open shared object file: No such file or directory123vim /etc/ld.so.conf /usr/local/cuda-7.0/lib64 #添加这行，具体看cuda版本ldconfig no module named cv2 没装python-opencv 1sudo apt-get install python-opencv 或者 1conda install -c https://conda.binstar.org/menpo opencv 或者将cv2.so copy到anaconda目录下 1cp /usr/lib/python2.7/dist-packages/cv2.so $ANACONDA_HOME/lib/python2.7/site-packages/ version `GLIBCXX_3.4.21’ not found12export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6echo "export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6" &gt;&gt; ~/.bashrc File already exists in database: caffe.proto 卸载protobuf，从’https://github.com/google/protobuf&#39;重新下载安装protobuf 1234./autogen.sh./configure --disable-shared CPPFLAGS="-fPIC"makesudo make install 重装opencv registry.count(type) == 0 (1 vs. 0) Layer type Convolution already registered. 有可能是在同时使用不同版本的caffe，注释掉后调用的caffe(include/layer_factory.hpp),或者全部caffe里面都注释，重新make。1234567static void AddCreator(const string&amp; type, Creator creator) &#123; CreatorRegistry&amp; registry = Registry(); //comment these 2 lines // CHECK_EQ(registry.count(type), 0) // &lt;&lt; "Layer type " &lt;&lt; type &lt;&lt; " already registered."; registry[type] = creator; &#125; fatal error: caffe/proto/caffe.pb.h: No such file or directory1chmod 777 -R caffe /usr/lib/x86_64-linux-gnu//libunwind.so.8: undefined reference to ‘lzma_index_uncompressed_size@XZ_5.0’ 查看liblzma的位置 123ldconfig -p | grep liblzma* #查看命令export LD_LIBRARY_PATH=/lib/x86_64-linux-gnu #在/lib下export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/ #也可能在/usr/lib下 从其他没问题的电脑copy所有liblzma*到上面得到的位置（网上其他方法试了不行，这个可以）。然后执行 1sudo ldconfig build_release/lib/libcaffe.so: undefined reference to ‘google::protobuf::io::CodedOutputStream::WriteVarint64ToArray(unsigned long long, unsigned char*)’ 用gcc5或者更新的编译器重新编译 cv::imread(cv::String const&amp;, int)’ .build_release/lib/libcaffe.so: undefined reference to cv::imencode(cv::String const&amp;, cv::_InputArray const&amp;, std::vector&amp;, std::vector const&amp;)’ 存在多个版本的opencv,查看使用哪个位置的opencv]]></content>
      <tags>
        <tag>caffe issues</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[处理hexo和MathJax兼容的问题]]></title>
    <url>%2F2017%2F06%2F04%2F%E5%A4%84%E7%90%86hexo%E5%92%8CMathJax%E5%85%BC%E5%AE%B9%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[处理hexo和MathJax兼容的问题问题及原因在用MathJax处理公式的时候发现一个问题，如果有两个含有下划线的公式，比如这句1a_t和b_t 会将其解析成1a&lt;em&gt;t和b&lt;/em&gt;t 原因是当hexo的markdown解析器遇到两个’_’时会自动解析成，与MathJax冲突。 改正方法在hexo的文件夹下找到marked.js，一般会在1hexo\node_modules\hexo-renderer-marked\node_modules\marked\lib\marked.js 在约459行找到该句1em: /^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 把’|’左边的去掉，改成：1em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 在约490行找到该句1em: /^_(?=\S)([\s\S]*?\S)_(?!_)|^\*(?=\S)([\s\S]*?\S)\*(?!\*)/ 把’|’左边的去掉，改成：1em: /^\*(?=\S)([\s\S]*?\S)\*(?!\*)/ 最后用1hexo clean &amp; hexo g 重新生成网页，公式解析正常。]]></content>
      <tags>
        <tag>hexo issues</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning]]></title>
    <url>%2F2017%2F06%2F03%2Fadnet%2F</url>
    <content type="text"><![CDATA[论文阅读：Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning本文使用深度强化学习来解决目标跟踪。项目地址:https://sites.google.com/view/cvpr2017-adnet 概述本文提出的算法可以利用深度强化学习逐帧地通过执行动作预测目标位置。相比于其它基于深度学习的方法，它具有计算量小，同时可以保持高的跟踪精度（体现在位置和尺度上）。其快速版本可以达到15FPS的跟踪速度。 问题设置 本文提出的ADNet网络架构如Fig.1。本文使用ADNet预测动作，根据预测的动作确定下一帧的目标的位置。本文使用Deep Reinforcement Learning的定义如下。 Action 本文使用了11个Action。分别是向左，向上，向右和向下四种方向以及2倍距离的这四种方向。其余三种Action分别是尺度变小，尺度变大，以及终止（在每一帧网络都会顺序输出若干个action，当action是终止时完成此帧的跟踪），如Fig.2所示。每个Action使用11维的向量进行one-hot编码，如\((1,0,0,0,0,0,0,0,0,0,0)\)，\((0,1,0,0,0,0,0,0,0,0,0)\)等。 State 本文状态\(s_t\)用一个元组表示,\(s_t = (p_t, d_t)\)。其中\( {p_t} \in {\mathbb{R}^{112 \times 112 \times 3}}\ \)表示由上一帧bounding box确定的图像块patch。\( d_t\)表示t时刻之前的\(k(=10)\)个action向量（由于目标运动的持续性，利用之前的action可以捕捉目标motion，同时可以避免振荡现象）。每个patch用4-d的向量\( b_t \)表示，\( {b_t} = [{x^{(t)}},{y^{(t)}},{w^{(t)}},{h^{(t)}}] \)。在t帧时图像块\( p_t \)根据 $${p_t} = \phi ({b_t},F)$$ 其中\(\phi \)表示预处理函数，即从图像\(F \)中截取\(b_t \)对应的图像块。 State transition function 状态转移函数状态转移函数包括两部分，图像块转移函数\(f_p \)和动态转移函数\(f_d \)。 下一个patch的位置根据 $$b_{t+1} = f_p(b_t,a_t)$$ 获得。 移动的单位大小是 $$\Delta {x^{(t)}} = \alpha {w^{(t)}}, \Delta {y^{(t)}} = \alpha {h^{(t)}}$$ 其中\(\alpha = 0.03\)，比如当action是’left’的时候，下一个patch的位置是\(b_{t+1} = [{x^{(t)}}-\Delta {x^{(t)}},{y^{(t)}},{w^{(t)}},{h^{(t)}}] \)；当action是’scale up’的时候，下一个patch的位置则是\(b_{t+1} = [{x^{(t)}},{y^{(t)}},{w^{(t)}}+\Delta {x^{(t)}},{h^{(t)}}+\Delta {y^{(t)}}]\)。 动态函数\(d_{t+1} = f_p(d_t,a_t)\)，表示action history。\(f_p\)其实就是一个队列。 Reward 回报函数定义成\(r(s)\)而不是\(r(s,a)\)因为每一帧都有若干个动作，最后一个动作\(a_T\)一定是’stop’，只需要看最后的状态即可。那么回报函数只看执行最后一个动作后的状态\(s_T\)，定义如下： \(G\)是groundtruth，\(IoU\)是交并比函数。跟踪score \(z_t\)定义成该帧终止时的reward \(r(s_T)\)，该score与平时接触到的跟踪算法一样是用来更新模型的。 Action-Decision Network本文使用训练好的VGG-M模型作为基础模型用来初始化卷积层部分。最后的两个卷积层fc6和fc7用来预测action的概率和状态的置信度。fc6输出各种action的概率，执行概率最高的action。当action是’stop’时完成当前帧的跟踪。注意到，因为每一帧都会执行若干个action，所以有可能出现振荡（oscillation）的现象。比如依次执行{left,right}或者{down,up}目标会来回移动。fc7输出当前状态是前景/背景的confidence，根据前者用来更新模型。 ADNet的训练ADNet的训练包括supervised learning 和reinforcement learning。使用supervised learning 训练使得能够预测出某个状态下的action，这一部分应该在单帧的图片完成的。在reinforcement learning中，网络通过在训练视频上的模拟跟踪以及利用action history来更新。通过这两部分训练的网络可以实现跟踪。 Supervised Learning（SL）SL的训练样本是图像块\( \{ p_j \} \)，action labels \( \{ o_j^{(act)} \} \)和class labels \( \{ o_j^{(cls)} \} \)。损失函数是一个多任务损失函数。定义如下 $${L_{SL}} = \frac{1}{m}\sum\limits_{j = 1}^m {L(o_j^{(act)},\hat o_j^{(act)})} + \frac{1}{m}\sum\limits_{j = 1}^m {L(o_j^{(cls)},\hat o_j^{(cls)})}$$ 带^的是预测值，其中两个真实值的定义如下： $$o_j^{(act)} = \arg \mathop {\max }\limits_a IoU(\bar f({p_j},a),G)$$ 就是说选择与groundtruth IoU最大的那个action（从11个action中选取）作为真实值。另外一个是 Reinforcement Learning (RL)在RL阶段，初始化的网络选用SL得到的网络，除了fc7外其余的参数全部需要训练。训练的时候，随机挑选一段序列)。通过跟踪模拟实现RL，一次模拟可以产生一系列状态\( \{ s_{t,l} \} \)，对应的actions \( \{ a_{t,l} \} \)以及rewards \( \{ r(s_{t,l}) \} \)，这里t表示帧内的time step，即每一帧采取一个action为一个time step，\(l\)是帧号。根据 $${a_{t,l}} = \arg \mathop {\max }\limits_a p(a|{s_{t,l}};{W_{RL}})$$ 得到状态\(s_{t,l}\)下的action \(a_{t,l}\)。当模拟跟踪结束时，根据groundtruth \( \{G\} \)计算跟踪score \( \{z_{t,l} \} \)，在模拟跟踪上， \(z_{t,l}=r(s_{T_l,l})\)只计算终止状态下的reward，跟踪到则reward为1，跟踪失败为-1。网络参数通过$$\Delta {W_{RL}} \propto \sum {\sum {\frac{\partial \log p({a_{t,l}}|{s_{t,l}};{W_{RL}})}{\partial {W_{RL}}}} } {z_{t,l}}$$ 最大化期望tracking score来更新。从更新情况可以看出，它的Loss Function为$${L_{RL}} \propto \sum {\sum {\log p({a_{t,l}}|{s_{t,l}};{W_{RL}})} } {z_{t,l}}$$ 就是最大化\(z_{t,l}\)。 RL训练的另一个好处是适用半监督学习的情况。当样本只有部分被标记时，只需要比较标记帧的跟踪情况，同样也可以训练。如下图所示，只有#160，#190和#220的帧有标记时，#190跟踪成功，161~190帧的tracking score都为+1，#220跟踪失败，那么191~220帧的tracking score都为-1。 在线自适应更新网络与最近许多工作类似，通过在当前位置采样进行网络fine-tuning。具体见论文（略）。 实验结果 结果还是很不错的。 Reference[1] Sangdoo et al. Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning[2] Visual Tracker Benchmark]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
</search>